{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U99DDmxPbBvl",
        "outputId": "c5d2d300-7f61-495e-f67b-832ec75659f2"
      },
      "id": "U99DDmxPbBvl",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "dA2kbD7obFBD"
      },
      "id": "dA2kbD7obFBD",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKV1WoQXbHL7",
        "outputId": "a604caf7-d45d-46ac-b01e-b79912b38fe2"
      },
      "id": "vKV1WoQXbHL7",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f21ad6-2345-46a6-9437-8a38f2a606a2",
      "metadata": {
        "id": "56f21ad6-2345-46a6-9437-8a38f2a606a2"
      },
      "source": [
        "### Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e10c754b-8fd3-49c3-bbad-04802ac2e6e6",
      "metadata": {
        "id": "e10c754b-8fd3-49c3-bbad-04802ac2e6e6"
      },
      "outputs": [],
      "source": [
        "import nltk, re, pprint, string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '')\n",
        "file = open('./dataset.txt', encoding = 'utf8').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc326db-1046-4694-b163-f1d623ce0b52",
      "metadata": {
        "id": "8dc326db-1046-4694-b163-f1d623ce0b52"
      },
      "source": [
        "### Preprocess of the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b577a120-b179-40e8-9966-ed7abaa508bf",
      "metadata": {
        "id": "b577a120-b179-40e8-9966-ed7abaa508bf"
      },
      "outputs": [],
      "source": [
        "file_nl_removed = \"\"\n",
        "for line in file:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")\n",
        "  file_nl_removed += line_nl_removed\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7e3178-07ff-4c5a-bafc-b6593d7cb398",
      "metadata": {
        "id": "8e7e3178-07ff-4c5a-bafc-b6593d7cb398"
      },
      "source": [
        "### Statistics of the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ffd609ca-b0f9-4d2b-90d2-b21d66f5f682",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffd609ca-b0f9-4d2b-90d2-b21d66f5f682",
        "outputId": "288ab685-1433-4187-effc-43d30bf1058f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences is 981\n",
            "The number of tokens is 27361\n",
            "The average number of tokens per sentence is 28\n",
            "The number of unique tokens are 3039\n"
          ]
        }
      ],
      "source": [
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents))\n",
        "\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words))\n",
        "\n",
        "average_tokens = round(len(words) / len(sents))\n",
        "print(\"The average number of tokens per sentence is\", average_tokens)\n",
        "\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTHTWiBMbkLz",
        "outputId": "15e24aa5-98d8-4630-e5b0-27d10e751d03"
      },
      "id": "sTHTWiBMbkLz",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "949b3288-62eb-4781-a007-4f66efbe5aed",
      "metadata": {
        "id": "949b3288-62eb-4781-a007-4f66efbe5aed"
      },
      "source": [
        "### Building the N-Gram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "399a4296-406f-4dad-b423-8071db5a820b",
      "metadata": {
        "id": "399a4296-406f-4dad-b423-8071db5a820b"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Now you can use stop_words to filter out stopwords in your text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6320c007-2a34-49b0-92cd-86ae664c9807",
      "metadata": {
        "id": "6320c007-2a34-49b0-92cd-86ae664c9807"
      },
      "outputs": [],
      "source": [
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "fourgram=[]\n",
        "tokenized_text = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fad18513-0f9d-4a3c-94f6-640397b9375f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fad18513-0f9d-4a3c-94f6-640397b9375f",
        "outputId": "876fa65f-2c38-41bb-a230-bba53ecfacf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams without stopword removal and without add-1 smoothing: \n",
            "\n",
            "Most common bigrams:  [(('said', 'the'), 209), (('said', 'alice'), 115), (('the', 'queen'), 65), (('the', 'king'), 60), (('a', 'little'), 59)]\n",
            "\n",
            "Most common trigrams:  [(('the', 'mock', 'turtle'), 51), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 21)]\n",
            "\n",
            "Most common fourgrams:  [(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('said', 'the', 'march', 'hare'), 8), (('will', 'you', 'wont', 'you'), 8)]\n"
          ]
        }
      ],
      "source": [
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence)\n",
        "    for word in sequence:\n",
        "        if (word =='.'):\n",
        "            sequence.remove(word)\n",
        "        else:\n",
        "            unigram.append(word)\n",
        "    tokenized_text.append(sequence)\n",
        "    bigram.extend(list(ngrams(sequence, 2)))\n",
        "    trigram.extend(list(ngrams(sequence, 3)))\n",
        "    fourgram.extend(list(ngrams(sequence, 4)))\n",
        "\n",
        "#removes ngrams containing only stopwords\n",
        "def removal(x):\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return(y)\n",
        "\n",
        "\n",
        "bigram = removal(bigram)\n",
        "trigram = removal(trigram)\n",
        "fourgram = removal(fourgram)\n",
        "freq_bi = nltk.FreqDist(bigram)\n",
        "freq_tri = nltk.FreqDist(trigram)\n",
        "freq_four = nltk.FreqDist(fourgram)\n",
        "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
        "print (\"Most common bigrams: \", freq_bi.most_common(5))\n",
        "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
        "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78a9bb2b-e158-4af7-8f85-30ba9274632c",
      "metadata": {
        "id": "78a9bb2b-e158-4af7-8f85-30ba9274632c"
      },
      "source": [
        "### Script for downloading the stopwords using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3ad94e87-881d-40d8-940e-4371072e97d0",
      "metadata": {
        "id": "3ad94e87-881d-40d8-940e-4371072e97d0"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6878bc8c-88f6-406e-81e8-4e2c14115e6f",
      "metadata": {
        "id": "6878bc8c-88f6-406e-81e8-4e2c14115e6f"
      },
      "source": [
        "### Print 10 Unigrams and Bigrams after removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b4cb73ff-2f70-41e3-874f-6d7598f70bd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4cb73ff-2f70-41e3-874f-6d7598f70bd9",
        "outputId": "91d348c3-e130-4aa2-d870-2c61f0412adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams with stopword removal and without add-1 smoothing: \n",
            "\n",
            "Most common unigrams:  [('said', 462), ('alice', 385), ('little', 128), ('one', 101), ('like', 85), ('know', 85), ('would', 83), ('went', 83), ('could', 77), ('thought', 74)]\n",
            "\n",
            "Most common bigrams:  [(('said', 'alice'), 122), (('mock', 'turtle'), 54), (('march', 'hare'), 31), (('said', 'king'), 29), (('thought', 'alice'), 26), (('white', 'rabbit'), 22), (('said', 'hatter'), 22), (('said', 'mock'), 20), (('said', 'caterpillar'), 18), (('said', 'gryphon'), 18)]\n"
          ]
        }
      ],
      "source": [
        "print(\"Most common n-grams with stopword removal and without add-1 smoothing: \\n\")\n",
        "unigram_sw_removed = [p for p in unigram if p not in stop_words]\n",
        "fdist = nltk.FreqDist(unigram_sw_removed)\n",
        "print(\"Most common unigrams: \", fdist.most_common(10))\n",
        "bigram_sw_removed = []\n",
        "bigram_sw_removed.extend(list(ngrams(unigram_sw_removed, 2)))\n",
        "fdist = nltk.FreqDist(bigram_sw_removed)\n",
        "print(\"\\nMost common bigrams: \", fdist.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5b3f4c8-3011-4959-8f0f-1c65c2ddcaa2",
      "metadata": {
        "id": "e5b3f4c8-3011-4959-8f0f-1c65c2ddcaa2"
      },
      "source": [
        "### Add-1 smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bdc7c175-0465-4fe3-94d8-e97b341d8b9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "bdc7c175-0465-4fe3-94d8-e97b341d8b9c",
        "outputId": "8f9a8643-b56d-402d-cf3c-0f4e96705469"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b26f077bc290>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mngrams_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mngrams_voc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtotal_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "ngrams_all = {1: [], 2: [], 3: [], 4: []}\n",
        "for i in range(4):\n",
        "    ngrams_all[i + 1] = [tuple(ngram) for ngram in ngrams(tokenized_text, i + 1)]\n",
        "\n",
        "ngrams_voc = {1: set(ngrams_all[1]), 2: set(ngrams_all[2]), 3: set(ngrams_all[3]), 4: set(ngrams_all[4])}\n",
        "\n",
        "total_ngrams = {i: len(ngrams_all[i]) for i in range(1, 5)}\n",
        "total_voc = {i: len(ngrams_voc[i]) for i in range(1, 5)}\n",
        "\n",
        "ngrams_prob = {1: [], 2: [], 3: [], 4: []}\n",
        "\n",
        "for i in range(1, 5):\n",
        "    ngram_counts = Counter(ngrams_all[i])\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        ngram_prob = (count + 1) / (total_ngrams[i] + total_voc[i])\n",
        "        ngrams_prob[i].append([ngram, ngram_prob])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765297a7-7c40-4917-a567-cec2aec09526",
      "metadata": {
        "id": "765297a7-7c40-4917-a567-cec2aec09526"
      },
      "source": [
        "### Prints top 10 unigram, bigram, trigram, fourgram after smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c3f76d6-7181-4c04-a882-9b85354f4b72",
      "metadata": {
        "id": "6c3f76d6-7181-4c04-a882-9b85354f4b72"
      },
      "outputs": [],
      "source": [
        "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "\n",
        "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
        "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))\n",
        "print (\"\\nMost common trigrams: \", str(ngrams_prob[3][:10]))\n",
        "print (\"\\nMost common fourgrams: \", str(ngrams_prob[4][:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa31df22-c5bf-4f26-b8f9-3a8282295b8a",
      "metadata": {
        "id": "aa31df22-c5bf-4f26-b8f9-3a8282295b8a"
      },
      "source": [
        "### Next word Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3ed262-c9f4-4ff9-90e4-4c2eebb20648",
      "metadata": {
        "id": "7a3ed262-c9f4-4ff9-90e4-4c2eebb20648"
      },
      "outputs": [],
      "source": [
        "str1 = 'after that alice said the'\n",
        "str2 = 'alice felt so desperate that she was'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90affe0-f0d2-4136-ad5b-fd32e56bb412",
      "metadata": {
        "id": "d90affe0-f0d2-4136-ad5b-fd32e56bb412",
        "outputId": "6deda78e-fec3-44ed-ca2d-89a47096a546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "String 1:  {1: ('the',), 2: ('said', 'the'), 3: ('alice', 'said', 'the')} \n",
            "String 2:  {1: ('was',), 2: ('she', 'was'), 3: ('that', 'she', 'was')}\n"
          ]
        }
      ],
      "source": [
        "token_1 = word_tokenize(str1)\n",
        "token_2 = word_tokenize(str2)\n",
        "ngram_1 = {1:[], 2:[], 3:[]}   #to store the n-grams formed\n",
        "ngram_2 = {1:[], 2:[], 3:[]}\n",
        "for i in range(3):\n",
        "    ngram_1[i+1] = list(ngrams(token_1, i+1))[-1]\n",
        "    ngram_2[i+1] = list(ngrams(token_2, i+1))[-1]\n",
        "print(\"String 1: \", ngram_1,\"\\nString 2: \",ngram_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3bf30d-722f-4508-847b-27d40de9250c",
      "metadata": {
        "id": "2d3bf30d-722f-4508-847b-27d40de9250c"
      },
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "\n",
        "pred_1 = {1:[], 2:[], 3:[]}\n",
        "for i in range(3):\n",
        "    count = 0\n",
        "    for each in ngrams_prob[i+2]:\n",
        "        if each[0][:-1] == ngram_1[i+1]:\n",
        "#to find predictions based on highest probability of n-grams\n",
        "\n",
        "            count +=1\n",
        "            pred_1[i+1].append(each[0][-1])\n",
        "            if count ==5:\n",
        "                break\n",
        "    if count<5:\n",
        "        while(count!=5):\n",
        "            pred_1[i+1].append(\"NOT FOUND\")\n",
        "#if no word prediction is found, replace with NOT FOUND\n",
        "            count +=1\n",
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "\n",
        "pred_2 = {1:[], 2:[], 3:[]}\n",
        "for i in range(3):\n",
        "    count = 0\n",
        "    for each in ngrams_prob[i+2]:\n",
        "        if each[0][:-1] == ngram_2[i+1]:\n",
        "            count +=1\n",
        "            pred_2[i+1].append(each[0][-1])\n",
        "            if count ==5:\n",
        "                break\n",
        "    if count<5:\n",
        "        while(count!=5):\n",
        "            pred_2[i+1].append(\"\\0\")\n",
        "            count +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511c6c3f-26a2-488f-bcbc-27923e5e0073",
      "metadata": {
        "id": "511c6c3f-26a2-488f-bcbc-27923e5e0073",
        "outputId": "03902633-084c-4e75-bde7-a50b34a2f30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\n",
            "\n",
            "String 1 - after that alice said the-\n",
            "\n",
            "Bigram model predictions: ['queen', 'king', 'gryphon', 'mock', 'hatter']\n",
            "Trigram model predictions: ['king', 'hatter', 'mock', 'caterpillar', 'gryphon']\n",
            "Fourgram model predictions: ['NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
            "\n",
            "String 2 - alice felt so desperate that she was-\n",
            "\n",
            "Bigram model predictions: ['a', 'the', 'not', 'that', 'going']\n",
            "Trigram model predictions: ['now', 'quite', 'a', 'beginning', 'walking']\n",
            "Fourgram model predictions: ['now', 'ready', 'quite', 'dozing', 'in']\n"
          ]
        }
      ],
      "source": [
        "print(\"Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\\n\")\n",
        "print(\"String 1 - after that alice said the-\\n\")\n",
        "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_1[1], pred_1[2], pred_1[3]))\n",
        "print(\"String 2 - alice felt so desperate that she was-\\n\")\n",
        "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\" .format(pred_2[1], pred_2[2], pred_2[3]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}